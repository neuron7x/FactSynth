#!/usr/bin/env python3
"""Validate benchmark results against regression thresholds.

This helper reads the JSON output produced by ``pytest --benchmark-json`` and
compares the measured latency and throughput with project-defined thresholds.
If any benchmark violates its configured limits the script exits with a non-zero
status code so CI can raise an alert.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Validate pytest-benchmark output")
    parser.add_argument(
        "--results",
        required=True,
        type=Path,
        help="Path to the JSON file generated by pytest-benchmark.",
    )
    parser.add_argument(
        "--thresholds",
        required=True,
        type=Path,
        help="Path to JSON file with latency/throughput thresholds.",
    )
    parser.add_argument(
        "--summary",
        type=Path,
        help="Optional path to write a Markdown summary of the benchmark run.",
    )
    return parser.parse_args()


def _load_json(path: Path) -> dict[str, Any]:
    if not path.exists():
        raise FileNotFoundError(f"Expected file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def format_float(value: float | None) -> str:
    if value is None:
        return "n/a"
    if value >= 100:
        return f"{value:,.0f}"
    if value >= 10:
        return f"{value:,.2f}"
    return f"{value:,.3f}"


def ensure_summary_dir(path: Path | None) -> None:
    if path and not path.parent.exists():
        path.parent.mkdir(parents=True, exist_ok=True)


def main() -> int:
    args = parse_args()
    results = _load_json(args.results)
    thresholds = _load_json(args.thresholds)

    benchmarks: list[dict[str, Any]] = results.get("benchmarks", [])
    if not benchmarks:
        raise SystemExit("No benchmark entries found in results file.")

    summary_rows: list[str] = [
        "# Benchmark results",
        "",
        "| Benchmark | Mean latency (ms) | Ops/sec | Max latency threshold (ms) | Min throughput threshold (ops/sec) | Status |",
        "| --- | --- | --- | --- | --- | --- |",
    ]

    failures: list[str] = []

    for bench in benchmarks:
        name: str = bench.get("fullname") or bench.get("name", "<unknown>")
        stats: dict[str, Any] = bench.get("stats", {})
        latency = float(stats.get("mean", 0.0)) * 1000.0
        ops: float | None = None
        ops_field = stats.get("ops")
        if isinstance(ops_field, dict):
            ops_value = ops_field.get("mean")
            if ops_value is not None:
                ops = float(ops_value)
        elif isinstance(ops_field, (int, float)):
            ops = float(ops_field)

        config = thresholds.get(name) or thresholds.get(bench.get("name", ""), {})
        max_latency = config.get("max_latency_ms") if isinstance(config, dict) else None
        min_throughput = config.get("min_throughput_rps") if isinstance(config, dict) else None

        status = "PASS"
        if max_latency is not None and latency > float(max_latency):
            status = "FAIL"
            failures.append(
                f"{name}: latency {latency:.3f}ms exceeds max {float(max_latency):.3f}ms"
            )
        if min_throughput is not None:
            if ops is None or ops < float(min_throughput):
                status = "FAIL"
                observed = "n/a" if ops is None else f"{ops:.3f}"
                failures.append(
                    f"{name}: throughput {observed}ops/s below min {float(min_throughput):.3f}ops/s"
                )

        summary_rows.append(
            "| {name} | {lat} | {ops} | {lat_thresh} | {ops_thresh} | {status} |".format(
                name=name,
                lat=format_float(latency),
                ops=format_float(ops),
                lat_thresh=format_float(float(max_latency)) if max_latency is not None else "n/a",
                ops_thresh=format_float(float(min_throughput))
                if min_throughput is not None
                else "n/a",
                status=status,
            )
        )

    if args.summary:
        ensure_summary_dir(args.summary)
        args.summary.write_text("\n".join(summary_rows) + "\n", encoding="utf-8")

    if failures:
        failure_message = "Benchmark regression detected:\n" + "\n".join(f"- {msg}" for msg in failures)
        raise SystemExit(failure_message)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
